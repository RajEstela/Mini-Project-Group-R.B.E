{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import randint\n",
    "\n",
    "# tree visualisation\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import randint\n",
    "\n",
    "# tree visualisation\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Cleaned Dataset\n",
    "\n",
    "Creating a dataframe from the cleaned dataset processed in [2-data-preparation-and-cleaning](2-data-preparation-and-cleaning.ipynb) jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = pd.read_csv('datasets/cleaned_dataset.csv', sep=',')\n",
    "display(cleaned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Problem Definition 1\n",
    "\n",
    "**Analyze official competitive matches and identify the best champion class combination that will result in a win.**\n",
    "\n",
    "To do so, we have combined the data sets from the Game Match Dataset and Champion Info Dataset and filtered it out to only take in teams that won.\n",
    "\n",
    "We then further merged the classes for each champion and inputted them in the dataframe. (e.g P1 Top (Name of Champion) P1 Top Class (Class type of champion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns_problem_1_team_1 = ['t1_result', 't1p1_champion', 't1p1_class', 't1p2_champion', 't1p2_class', 't1p3_champion', 't1p3_class', 't1p4_champion', 't1p4_class', 't1p5_champion', 't1p5_class']\n",
    "desired_columns_problem_1_team_2 = ['t2_result', 't2p1_champion', 't2p1_class', 't2p2_champion', 't2p2_class', 't2p3_champion', 't2p3_class', 't2p4_champion', 't2p4_class', 't2p5_champion', 't2p5_class']\n",
    "\n",
    "problem_1_data_team_1 = pd.DataFrame(cleaned_dataset, columns=desired_columns_problem_1_team_1)\n",
    "problem_1_data_team_2 = pd.DataFrame(cleaned_dataset, columns=desired_columns_problem_1_team_2)\n",
    "\n",
    "problem_1_data_team_1.columns = ['result', 'p1_champion', 'p1_class', 'p2_champion', 'p2_class', 'p3_champion', 'p3_class', 'p4_champion', 'p4_class', 'p5_champion', 'p5_class']\n",
    "problem_1_data_team_2.columns = ['result', 'p1_champion', 'p1_class', 'p2_champion', 'p2_class', 'p3_champion', 'p3_class', 'p4_champion', 'p4_class', 'p5_champion', 'p5_class']\n",
    "\n",
    "problem_1_data_all_team = pd.concat([problem_1_data_team_1, problem_1_data_team_2], ignore_index=True)\n",
    "\n",
    "del problem_1_data_team_1\n",
    "del problem_1_data_team_2\n",
    "\n",
    "problem_1_data_winning_team = problem_1_data_all_team[problem_1_data_all_team['result'] == 1]\n",
    "display(problem_1_data_winning_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_1_data_winning_team.drop('result', axis=1, inplace=True)\n",
    "problem_1_data_winning_team.columns = ['P1 Top', 'P1 Top Class', 'P2 Jungle', 'P2 Jungle Class', 'P3 Mid', 'P3 Mid Class', 'P4 Bot','P4 Bot Class','P5 Support','P5 Support Class']\n",
    "\n",
    "display(problem_1_data_winning_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = problem_1_data_winning_team.copy()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the prepared data frame after doing the successfuly merging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframe with team compositions\n",
    "data = merged_data[['P1 Top Class', 'P2 Jungle Class', 'P3 Mid Class', 'P4 Bot Class', 'P5 Support Class']]\n",
    "dfs = pd.DataFrame(data)\n",
    "# Fill any NaN values with a placeholder string\n",
    "dfs = dfs.fillna('Unknown')\n",
    "\n",
    "# Convert all values to strings\n",
    "dfs = dfs.astype(str)\n",
    "\n",
    "# Mapping dictionary to replace class names with letters\n",
    "class_mapping = {\n",
    "    'Assassin': 'A',\n",
    "    'Assassin-Warrior': 'AW',\n",
    "    'Mage': 'M',\n",
    "    'Mage-Assassin': 'MA',\n",
    "    'Mage-Marksman': 'MM',\n",
    "    'Mage-Support': 'MS',\n",
    "    'Mage-Warrior': 'MW',\n",
    "    'Marksman': 'MK',\n",
    "    'Marksman-Support': 'MKS',\n",
    "    'Marksman-Warrior': 'MKW',\n",
    "    'Support': 'S',\n",
    "    'Tank': 'T',\n",
    "    'Tank-Support': 'TS',\n",
    "    'Tank-Warrior': 'TW',\n",
    "    'Warrior': 'W',\n",
    "    'Warrior-Assassin': 'WA'\n",
    "}\n",
    "\n",
    "# Dictionary to map class abbreviations back to full names\n",
    "class_mapping_reverse = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "# Rename class columns with letters\n",
    "for col in dfs.columns:\n",
    "    dfs[col] = dfs[col].map(class_mapping)\n",
    "\n",
    "# Concatenate the class columns to create a new column representing the class set\n",
    "dfs['Class_Set'] = dfs.apply(lambda row: '-'.join(row), axis=1)\n",
    "\n",
    "# Count the occurrences of each class set\n",
    "top_class_sets = dfs['Class_Set'].value_counts().head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(16, 26))  # Increase the height to accommodate the label below\n",
    "top_class_sets.plot(kind='bar')\n",
    "plt.title('Frequency of Champion Class Sets')\n",
    "plt.xlabel('Champion Class Set')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, fontsize=12)  # Adjust the rotation and fontsize as needed\n",
    "\n",
    "# Display class_mapping_reverse at the bottom left of the plot\n",
    "table_text = '\\n'.join([f'{abbr}: {full}' for abbr, full in class_mapping_reverse.items()])\n",
    "plt.text(0, -0.2, table_text, ha='left', fontsize=10, transform=plt.gca().transAxes)\n",
    "\n",
    "# Find the class set with the highest frequency\n",
    "best_class_set = top_class_sets.idxmax()\n",
    "best_frequency = top_class_sets.max()\n",
    "\n",
    "# Add text on the top right\n",
    "bot_right_text = f'Best class set: {best_class_set}\\nFrequency: {best_frequency}'\n",
    "plt.text(1, -0.1, bot_right_text, ha='right', va='top', fontsize=12, transform=plt.gca().transAxes)\n",
    "\n",
    "top_right_text = f'Refers to the position of the Champions, which the position matters'\n",
    "plt.text(0.99, 0.95, top_right_text, ha='right', va='top', fontsize=12, transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best champion class set is: W-W-M-MK-S\n",
    "#### Frequency of occurrence: 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = merged_data[['P1 Top Class', 'P2 Jungle Class', 'P3 Mid Class', 'P4 Bot Class', 'P5 Support Class']]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate class frequencies for each position\n",
    "class_frequencies = df.apply(pd.Series.value_counts)\n",
    "\n",
    "# Transpose the DataFrame for better visualization\n",
    "class_frequencies = class_frequencies.T\n",
    "\n",
    "# Reset index to make position a column instead of index\n",
    "class_frequencies.reset_index(inplace=True)\n",
    "\n",
    "# Melt the DataFrame to convert it to long format for plotting\n",
    "class_frequencies_melted = pd.melt(class_frequencies, id_vars=['index'], var_name='Class', value_name='Frequency')\n",
    "\n",
    "# Filter data for each position\n",
    "df_p1 = class_frequencies_melted[class_frequencies_melted['index'] == 'P1 Top Class']\n",
    "df_p2 = class_frequencies_melted[class_frequencies_melted['index'] == 'P2 Jungle Class']\n",
    "df_p3 = class_frequencies_melted[class_frequencies_melted['index'] == 'P3 Mid Class']\n",
    "df_p4 = class_frequencies_melted[class_frequencies_melted['index'] == 'P4 Bot Class']\n",
    "df_p5 = class_frequencies_melted[class_frequencies_melted['index'] == 'P5 Support Class']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(5, 1, figsize=(16, 24), sharex=True)\n",
    "\n",
    "# Plot for P1 Top Class\n",
    "sns.scatterplot(data=df_p1, x='Class', y='Frequency', hue='Class', s=200, ax=axes[0])\n",
    "axes[0].set_title('Distribution of Class Frequencies For P1 Top Position')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for P2 Jungle Class\n",
    "sns.scatterplot(data=df_p2, x='Class', y='Frequency', hue='Class', s=200, ax=axes[1])\n",
    "axes[1].set_title('Distribution of Class Frequencies For P2 Jungle Position')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for P3 Mid Class\n",
    "sns.scatterplot(data=df_p3, x='Class', y='Frequency', hue='Class', s=200, ax=axes[2])\n",
    "axes[2].set_title('Distribution of Class Frequencies For P3 Mid Position')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for P4 Bot Class\n",
    "sns.scatterplot(data=df_p4, x='Class', y='Frequency', hue='Class', s=200, ax=axes[3])\n",
    "axes[3].set_title('Distribution of Class Frequencies For P4 Bot Position')\n",
    "axes[3].set_xlabel('Class')\n",
    "axes[3].set_ylabel('Frequency')\n",
    "axes[3].legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot for P5 Support Class\n",
    "sns.scatterplot(data=df_p5, x='Class', y='Frequency', hue='Class', s=200, ax=axes[4])\n",
    "axes[4].set_title('Distribution of Class Frequencies For P5 Support Position')\n",
    "axes[4].set_xlabel('Class')\n",
    "axes[4].set_ylabel('Frequency')\n",
    "axes[4].legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[4].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame\n",
    "heatmap_data = class_frequencies_melted.pivot_table(index='index', columns='Class', values='Frequency', fill_value=0)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, cmap='viridis', annot=True, fmt='.0f')  # Use '.0f' for floats\n",
    "plt.title('Class Frequency Across Positions')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Position')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each position\n",
    "positions = class_frequencies_melted['index'].unique()\n",
    "for position in positions:\n",
    "    # Filter data for the current position\n",
    "    df_position = class_frequencies_melted[class_frequencies_melted['index'] == position]\n",
    "    \n",
    "    # Calculate total frequencies for each class for the current position\n",
    "    total_class_frequencies = df_position.groupby('Class')['Frequency'].sum()\n",
    "    \n",
    "    # Sort the class frequencies in descending order and select the top 5 classes\n",
    "    top_5_classes = total_class_frequencies.sort_values(ascending=False).head(5)\n",
    "    \n",
    "    # Plot the pie chart for the top 5 classes in the current position\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.pie(top_5_classes, labels=top_5_classes.index, autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(f'Top 5 Class Frequencies for {position}')\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Predictor Based on Champion Class and Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2024_LoL_esports_match_data_from_OraclesElixir_gamedata_v4.csv\n",
    "all_data = pd.read_csv('datasets/2024_LoL_esports_match_data_from_OraclesElixir_gamedata.csv', sep=',')\n",
    "all_data.head()\n",
    "\n",
    "clean_data = pd.DataFrame(all_data[['t1_result', 't2_result', 't1p1_champion', 't1p2_champion', 't1p3_champion', 't1p4_champion', 't1p5_champion', 't2p1_champion', 't2p2_champion', 't2p3_champion', 't2p4_champion', 't2p5_champion']])\n",
    "clean_data.head()\n",
    "\n",
    "# Response\n",
    "team1 = pd.DataFrame(clean_data[['t1_result', 't1p1_champion', 't1p2_champion', 't1p3_champion', 't1p4_champion', 't1p5_champion']])\n",
    "# Predictor\n",
    "team2 = pd.DataFrame(clean_data[['t2_result', 't2p1_champion', 't2p2_champion', 't2p3_champion', 't2p4_champion', 't2p5_champion']])\n",
    "\n",
    "team1.columns = ['result', 'p1', 'p2', 'p3', 'p4', 'p5']\n",
    "team2.columns = ['result', 'p1', 'p2', 'p3', 'p4', 'p5']\n",
    "\n",
    "all_team = pd.concat([team1, team2], ignore_index=True)\n",
    "display(all_team)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# LOL-Champions.csv\n",
    "champion_data = pd.read_csv('datasets/LoL-Champions.csv')\n",
    "clean_champion_data = pd.DataFrame(champion_data[['Name', 'Class']])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Merging both data sets\n",
    "merged_data = pd.merge(all_team, clean_champion_data, left_on='p1', right_on='Name', how='left')\n",
    "merged_data.rename(columns={'Class': 'p1_class'}, inplace=True)\n",
    "merged_data.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "merged_data = pd.merge(merged_data, clean_champion_data, left_on='p2', right_on='Name', how='left')\n",
    "merged_data.rename(columns={'Class': 'p2_class'}, inplace=True)\n",
    "merged_data.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "merged_data = pd.merge(merged_data, clean_champion_data, left_on='p3', right_on='Name', how='left')\n",
    "merged_data.rename(columns={'Class': 'p3_class'}, inplace=True)\n",
    "merged_data.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "merged_data = pd.merge(merged_data, clean_champion_data, left_on='p4', right_on='Name', how='left')\n",
    "merged_data.rename(columns={'Class': 'p4_class'}, inplace=True)\n",
    "merged_data.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "merged_data = pd.merge(merged_data, clean_champion_data, left_on='p5', right_on='Name', how='left')\n",
    "merged_data.rename(columns={'Class': 'p5_class'}, inplace=True)\n",
    "merged_data.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "columns_order = ['result', 'p1', 'p1_class', 'p2', 'p2_class', 'p3', 'p3_class', 'p4', 'p4_class', 'p5', 'p5_class']\n",
    "final_clean_team_data = merged_data[columns_order]\n",
    "\n",
    "display(merged_data)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Prediction\n",
    "\n",
    "# y = pd.DataFrame(final_clean_team_data[['p1_class', 'p2_class', 'p3_class', 'p4_class', 'p5_class']])\n",
    "# X = pd.DataFrame(final_clean_team_data[['result', 'p1_class', 'p2_class', 'p3_class', 'p4_class', 'p5_class']])\n",
    "\n",
    "X = pd.DataFrame(final_clean_team_data[['p1_class', 'p2_class', 'p3_class', 'p4_class', 'p5_class']])\n",
    "y = pd.DataFrame(final_clean_team_data[['result']])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()\n",
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "categorical_columns = ['p1_class', 'p2_class', 'p3_class', 'p4_class', 'p5_class']\n",
    "\n",
    "X = pd.DataFrame(final_clean_team_data[['p1_class', 'p2_class', 'p3_class', 'p4_class', 'p5_class']])\n",
    "y = pd.DataFrame(final_clean_team_data[['result']])\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Append regression model to preprocessing pipeline\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", model.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_train, y_train_pred))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", model.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_test, y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "data_encoded = pd.get_dummies(final_clean_team_data)\n",
    "\n",
    "X = data_encoded.drop('result', axis=1)\n",
    "y = data_encoded['result']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", model.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_train, y_train_pred))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", model.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_test, y_test_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both version of linear regresion has negative Explained Variance. This could mean the following:\n",
    "- Team composition & result (win/loss) is not linear\n",
    "- Data is overfitted\n",
    "- Inadequate Features\n",
    "- Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='p1', y='result', data=merged_data, label='Position 1')\n",
    "sns.scatterplot(x='p2', y='result', data=merged_data, label='Position 2')\n",
    "sns.scatterplot(x='p3', y='result', data=merged_data, label='Position 3')\n",
    "sns.scatterplot(x='p4', y='result', data=merged_data, label='Position 4')\n",
    "sns.scatterplot(x='p5', y='result', data=merged_data, label='Position 5')\n",
    "\n",
    "plt.title('Team Composition vs. Game Result')\n",
    "plt.xlabel('Champion')\n",
    "plt.ylabel('Game Result (Win: 1, Loss: 0)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above diagram, we learn that Team composition & result (win/loss) is not linear.\n",
    "\n",
    "After doing some research, we could use Random Trees predicting Binary values.\n",
    "\n",
    "We will show that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Problem Definition 2\n",
    "\n",
    "What is the most optimal and suitable game variable in predicting the total combined gold value? Amongst these variables total kills, total minion score, neutral objectives and structures taken down.\n",
    "\n",
    "The table below shows the variables and their corresponding columns in the datasets.\n",
    "\n",
    "| Variables | Columns in Dataset |\n",
    "| - | - |\n",
    "| Total Combined Gold | `t1_totalgold`, `t2_totalgold` |\n",
    "| Total Kills | `t1_kills`, `t2_kills` |\n",
    "| Total Minion Score | `t1_totalcs`, `t2_totalcs` |\n",
    "| Neutral Objectives | `t1_barons`, `t2_barons`, `t1_dragons`, `t2_dragons`, `t1_heralds`, `t2_heralds` |\n",
    "| Structures | `t1_towers`, `t2_towers`, `t1_inhibitors`, `t2_inhibitors` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Cleaned Dataset\n",
    "\n",
    "Processing the cleaned dataset for this problem definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns_problem_2 = ['t1_totalgold', 't2_totalgold', 't1_barons', 't2_barons', 't1_dragons', 't2_dragons', 't1_heralds', 't2_heralds', 't1_towers', 't2_towers', 't1_inhibitors', 't2_inhibitors', 't1_kills', 't2_kills', 't1_totalcs', 't2_totalcs']\n",
    "\n",
    "problem_2_data = pd.DataFrame(cleaned_dataset, columns=desired_columns_problem_2)\n",
    "display(problem_2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further processing to combine the team 1 (start with t1) and team 2 (start with t2) columns into a single column (example `t1_totalgold` and `t2_totalgold` into `totalgold`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine team 1 and team 2 values for each column\n",
    "for column in problem_2_data.columns:\n",
    "    if column.startswith('t1'):\n",
    "        corresponding_column = 't2' + column[2:]\n",
    "        if corresponding_column in problem_2_data.columns:\n",
    "            problem_2_data[column] += problem_2_data[corresponding_column]\n",
    "\n",
    "# drop team 2 columns\n",
    "problem_2_data_processed = problem_2_data[[col for col in problem_2_data.columns if not col.startswith('t2')]]\n",
    "\n",
    "# rename columns by removing 't1' prefix\n",
    "problem_2_data_processed.columns = [col.replace('t1_', '') for col in problem_2_data_processed.columns]\n",
    "\n",
    "# delete problem_2_data dataframe to release memory\n",
    "del problem_2_data\n",
    "\n",
    "# create a copy of problem_2_data_processed\n",
    "problem_2_data = problem_2_data_processed.copy()\n",
    "\n",
    "# combine barons, dragons, and heralds into neutralObjectives in problem_2_data\n",
    "problem_2_data['neutralObjectives'] = problem_2_data_processed['barons'] + problem_2_data_processed['dragons'] + problem_2_data_processed['heralds']\n",
    "\n",
    "# combine towers and inhibitors into structures in problem_2_data\n",
    "problem_2_data['structures'] = problem_2_data_processed['towers'] + problem_2_data_processed['inhibitors']\n",
    "\n",
    "# drop barons, dragons, heralds, towers and inhibitors from problem_2_data\n",
    "problem_2_data.drop(columns=['barons', 'dragons', 'heralds', 'towers', 'inhibitors'], inplace=True)\n",
    "\n",
    "# delete problem_2_data_processed dataframe to release memory\n",
    "del problem_2_data_processed\n",
    "\n",
    "display(problem_2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot, Histograms and Violin Plot\n",
    "\n",
    "Plotting box plot, histograms and violin plot to better understand the distribution of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframes from the columns related to the variables mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_2_data_totalgold = pd.DataFrame(problem_2_data['totalgold'])\n",
    "problem_2_data_kills = pd.DataFrame(problem_2_data['kills'])\n",
    "problem_2_data_totalcs = pd.DataFrame(problem_2_data['totalcs'])\n",
    "problem_2_data_neutralObjectives = pd.DataFrame(problem_2_data['neutralObjectives'])\n",
    "problem_2_data_structures = pd.DataFrame(problem_2_data['structures'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable 'totalgold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib figure with three subplots\n",
    "f, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# plot the basic uni-variate figures for totalgold\n",
    "sns.boxplot(data = problem_2_data_totalgold, orient = \"h\", ax = axes[0])\n",
    "sns.histplot(data = problem_2_data_totalgold, ax = axes[1])\n",
    "sns.violinplot(data = problem_2_data_totalgold, orient = \"h\", ax = axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable 'kills'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib figure with three subplots\n",
    "f, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# plot the basic uni-variate figures for kills\n",
    "sns.boxplot(data = problem_2_data_kills, orient = \"h\", ax = axes[0])\n",
    "sns.histplot(data = problem_2_data_kills, ax = axes[1])\n",
    "sns.violinplot(data = problem_2_data_kills, orient = \"h\", ax = axes[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable 'totalcs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib figure with three subplots\n",
    "f, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# plot the basic uni-variate figures for totalcs\n",
    "sns.boxplot(data = problem_2_data_totalcs, orient = \"h\", ax = axes[0])\n",
    "sns.histplot(data = problem_2_data_totalcs, ax = axes[1])\n",
    "sns.violinplot(data = problem_2_data_totalcs, orient = \"h\", ax = axes[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable 'neutralObjectives'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib figure with three subplots\n",
    "f, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# plot the basic uni-variate figures for neutralObjectives\n",
    "sns.boxplot(data = problem_2_data_neutralObjectives, orient = \"h\", ax = axes[0])\n",
    "sns.histplot(data = problem_2_data_neutralObjectives, ax = axes[1])\n",
    "sns.violinplot(data = problem_2_data_neutralObjectives, orient = \"h\", ax = axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable 'structures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib figure with three subplots\n",
    "f, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# plot the basic uni-variate figures for structures\n",
    "sns.boxplot(data = problem_2_data_structures, orient = \"h\", ax = axes[0])\n",
    "sns.histplot(data = problem_2_data_structures, ax = axes[1])\n",
    "sns.violinplot(data = problem_2_data_structures, orient = \"h\", ax = axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Plot\n",
    "\n",
    "Plotting joint plot to better understand the relationships between the totalgold and the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables 'totalgold' and 'kills'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a joint dataframe by concatenating the two variables\n",
    "problem_2_data_totalgold_kills = pd.concat([problem_2_data_totalgold, problem_2_data_kills], axis = 1).reindex(problem_2_data_totalgold.index)\n",
    "\n",
    "# set the style of seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# create the jointplot with adjusted height and aspect ratio\n",
    "sns.jointplot(data=problem_2_data_totalgold_kills, x=\"totalgold\", y=\"kills\", height=10, ratio=3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# delete dataframe to release memory\n",
    "del problem_2_data_totalgold_kills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot is showing data points that are clustedred around the middle-lower left area, showing that there are some relationship between 'totalgold' and 'kills', but it may not be a good predicter to predict 'totalgold'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables 'totalgold' and 'totalcs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a joint dataframe by concatenating the two variables\n",
    "problem_2_data_totalgold_totalcs = pd.concat([problem_2_data_totalgold, problem_2_data_totalcs], axis = 1).reindex(problem_2_data_totalgold.index)\n",
    "\n",
    "# set the style of seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# create the jointplot with adjusted height and aspect ratio\n",
    "sns.jointplot(data=problem_2_data_totalgold_totalcs, x=\"totalgold\", y=\"totalcs\", height=10, ratio=3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# delete dataframe to release memory\n",
    "del problem_2_data_totalgold_totalcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot is showing data points that almost appear to be in a straight line, this shows the relationship between 'totalgold' and 'totalcs' are quite strong and 'totalcs' may be the a very good predicter for 'totalgold' when comapred to 'kills'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables 'totalgold' and 'neutralObjectives'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a joint dataframe by concatenating the two variables\n",
    "problem_2_data_totalgold_neutralObjectives = pd.concat([problem_2_data_totalgold, problem_2_data_neutralObjectives], axis = 1).reindex(problem_2_data_totalgold.index)\n",
    "\n",
    "# set the style of seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# create the jointplot with adjusted height and aspect ratio\n",
    "sns.jointplot(data=problem_2_data_totalgold_neutralObjectives, x=\"totalgold\", y=\"neutralObjectives\", height=10, ratio=3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# delete dataframe to release memory\n",
    "del problem_2_data_totalgold_neutralObjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot is showing data points that appear to be heading towards the same direction in a straight line, but the data points on the x-axis is spread out very widely. It indicates that there are some relationship between 'totalgold' and 'neutralObjectives', but it may not be a good predicter to predict 'totalgold'. However, it appear to be a better predictor than 'kills'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables 'totalgold' and 'structures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a joint dataframe by concatenating the two variables\n",
    "problem_2_data_totalgold_structures = pd.concat([problem_2_data_totalgold, problem_2_data_structures], axis = 1).reindex(problem_2_data_totalgold.index)\n",
    "\n",
    "# set the style of seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# create the jointplot with adjusted height and aspect ratio\n",
    "sns.jointplot(data=problem_2_data_totalgold_structures, x=\"totalgold\", y=\"structures\", height=10, ratio=3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# delete dataframe to release memory\n",
    "del problem_2_data_totalgold_structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot is showing data points that appear all over the plot. It shows that there is a very weak relationship between 'totalgold' and 'neutralObjectives', or almost none at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 4 variables, 'totalcs' appears to be the best predictor for 'totalgold'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "We will use linear regression to confirm out finding from joint plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential models and functions from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict 'totalgold' Using 'kills'\n",
    "\n",
    "| Predictor (X) | Response (y) |\n",
    "| - | - |\n",
    "| `kills` | `totalgold` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits data points into random train and test subsets\n",
    "# but setting random_state for the markdown\n",
    "X_train, X_test, y_train, y_test = train_test_split(problem_2_data_kills, problem_2_data_totalgold, test_size=0.25, random_state=100)\n",
    "\n",
    "# linear regression using train data\n",
    "linreg = LinearRegression() # create the linear regression object\n",
    "linreg.fit(X_train, y_train) # train the linear regression model\n",
    "\n",
    "# coefficients of the linear regression line\n",
    "print('Intercept of Regression \\t: b = ', linreg.intercept_)\n",
    "print('Coefficients of Regression \\t: a = ', linreg.coef_)\n",
    "print()\n",
    "\n",
    "# predict totalgold values corresponding to kills\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "y_test_pred = linreg.predict(X_test)\n",
    "\n",
    "# goodness of fit (on train data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_train, y_train_pred))\n",
    "print()\n",
    "\n",
    "# goodness of fit (on test data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "# plot the predictions vs the true values\n",
    "f, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "# train data\n",
    "axes[0].scatter(y_train, y_train_pred, color = \"blue\")\n",
    "axes[0].plot(y_train, y_train, color = \"red\", linewidth = 2)\n",
    "axes[0].set_xlabel(\"True values of the Response Variable (Train)\")\n",
    "axes[0].set_ylabel(\"Predicted values of the Response Variable (Train)\")\n",
    "\n",
    "# test data\n",
    "axes[1].scatter(y_test, y_test_pred, color = \"green\")\n",
    "axes[1].plot(y_train, y_train, color = \"red\", linewidth = 2)\n",
    "axes[1].set_xlabel(\"True values of the Response Variable (Test)\")\n",
    "axes[1].set_ylabel(\"Predicted values of the Response Variable (Test)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the explained variance and the mean squared error for both train and test dataset are similiar and consistent.\n",
    "\n",
    "However, with a low explained variance (train: 0.197, test: 0.242) and high mean squared error (train: 331,827,322, test: 304,585,263), this shows that 'kills' is not a good predictor for 'totalgold'.\n",
    "\n",
    "This is the same finding as the finding from the joint plot section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict 'totalgold' Using 'totalcs'\n",
    "\n",
    "| Predictor (X) | Response (y) |\n",
    "| - | - |\n",
    "| `totalcs` | `totalgold` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits data points into random train and test subsets\n",
    "# but setting random_state for the markdown\n",
    "X_train, X_test, y_train, y_test = train_test_split(problem_2_data_totalcs, problem_2_data_totalgold, test_size=0.25, random_state=100)\n",
    "\n",
    "# linear regression using train data\n",
    "linreg = LinearRegression() # create the linear regression object\n",
    "linreg.fit(X_train, y_train) # train the linear regression model\n",
    "\n",
    "# coefficients of the linear regression line\n",
    "print('Intercept of Regression \\t: b = ', linreg.intercept_)\n",
    "print('Coefficients of Regression \\t: a = ', linreg.coef_)\n",
    "print()\n",
    "\n",
    "# predict totalgold values corresponding to totalcs\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "y_test_pred = linreg.predict(X_test)\n",
    "\n",
    "# goodness of fit (on train data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_train, y_train_pred))\n",
    "print()\n",
    "\n",
    "# goodness of fit (on test data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "# plot the predictions vs the true values\n",
    "f, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "# train data\n",
    "axes[0].scatter(y_train, y_train_pred, color = \"blue\")\n",
    "axes[0].plot(y_train, y_train, color = \"red\", linewidth = 2)\n",
    "axes[0].set_xlabel(\"True values of the Response Variable (Train)\")\n",
    "axes[0].set_ylabel(\"Predicted values of the Response Variable (Train)\")\n",
    "\n",
    "# test data\n",
    "axes[1].scatter(y_test, y_test_pred, color = \"green\")\n",
    "axes[1].plot(y_train, y_train, color = \"red\", linewidth = 2)\n",
    "axes[1].set_xlabel(\"True values of the Response Variable (Test)\")\n",
    "axes[1].set_ylabel(\"Predicted values of the Response Variable (Test)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the explained variance and the mean squared error for both train and test dataset are similiar and consistent.\n",
    "\n",
    "And with a significant higher explained variance (train: 0.856, test: 0.839) and lower mean squared error (train: 59,328,416, test: 64,692,970), this shows that 'totalcs' is a very good predictor for 'totalgold' when compared to 'kills'.\n",
    "\n",
    "This is the same finding as the finding from the joint plot section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict 'totalgold' Using 'neutralObjectives'\n",
    "\n",
    "| Predictor (X) | Response (y) |\n",
    "| - | - |\n",
    "| `neutralObjectives` | `totalgold` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits data points into random train and test subsets\n",
    "# but setting random_state for the markdown\n",
    "X_train, X_test, y_train, y_test = train_test_split(problem_2_data_neutralObjectives, problem_2_data_totalgold, test_size=0.25, random_state=100)\n",
    "\n",
    "# linear regression using train data\n",
    "linreg = LinearRegression()  # create the linear regression object\n",
    "linreg.fit(X_train, y_train) # train the linear regression model\n",
    "\n",
    "# coefficients of the linear regression line\n",
    "print('Intercept of Regression \\t: b = ', linreg.intercept_)\n",
    "print('Coefficients of Regression \\t: a = ', linreg.coef_)\n",
    "print()\n",
    "\n",
    "# predict totalgold values corresponding to neutralObjectives\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "y_test_pred = linreg.predict(X_test)\n",
    "\n",
    "# goodness of fit (on train data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_train, y_train_pred))\n",
    "print()\n",
    "\n",
    "# goodness of fit (on test data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "# plot the predictions vs the true values\n",
    "f, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "# train data\n",
    "axes[0].scatter(y_train, y_train_pred, color = \"blue\")\n",
    "axes[0].plot(y_train, y_train, color = \"red\", linewidth = 2)\n",
    "axes[0].set_xlabel(\"True values of the Response Variable (Train)\")\n",
    "axes[0].set_ylabel(\"Predicted values of the Response Variable (Train)\")\n",
    "\n",
    "# test data\n",
    "axes[1].scatter(y_test, y_test_pred, color = \"green\")\n",
    "axes[1].plot(y_train, y_train, color = \"red\", linewidth = 2)\n",
    "axes[1].set_xlabel(\"True values of the Response Variable (Test)\")\n",
    "axes[1].set_ylabel(\"Predicted values of the Response Variable (Test)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the explained variance and the mean squared error for both train and test dataset are similiar and consistent.\n",
    "\n",
    "The explained variance (train: 0.642, test: 0.598) and mean squared error (train: 147,701,779, test: 161,345,899) are also better than 'kills', but worse then 'totalcs'. This shows that 'neutralObjectives' can be used to predict 'totalgold', but 'totalcs' is still a better predictor.\n",
    "\n",
    "This is the same finding as the finding from the joint plot section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict 'totalgold' Using 'structures'\n",
    "\n",
    "| Predictor (X) | Response (y) |\n",
    "| - | - |\n",
    "| `structures` | `totalgold` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits data points into random train and test subsets\n",
    "# but setting random_state for the markdown\n",
    "X_train, X_test, y_train, y_test = train_test_split(problem_2_data_structures, problem_2_data_totalgold, test_size=0.25, random_state=100)\n",
    "\n",
    "# linear regression using train data\n",
    "linreg = LinearRegression() # create the linear regression object\n",
    "linreg.fit(X_train, y_train) # train the linear regression model\n",
    "\n",
    "# coefficients of the linear regression line\n",
    "print('Intercept of Regression \\t: b = ', linreg.intercept_)\n",
    "print('Coefficients of Regression \\t: a = ', linreg.coef_)\n",
    "print()\n",
    "\n",
    "# predict totalgold values corresponding to structures\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "y_test_pred = linreg.predict(X_test)\n",
    "\n",
    "# goodness of fit (on train data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_train, y_train))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_train, y_train_pred))\n",
    "print()\n",
    "\n",
    "# goodness of fit (on test data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_test, y_test))\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mean_squared_error(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "# plot the predictions vs the true values\n",
    "f, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "# train data\n",
    "axes[0].scatter(y_train, y_train_pred, color = \"blue\")\n",
    "axes[0].plot(y_train, y_train, color = \"red\", linewidth = 2)\n",
    "axes[0].set_xlabel(\"True values of the Response Variable (Train)\")\n",
    "axes[0].set_ylabel(\"Predicted values of the Response Variable (Train)\")\n",
    "\n",
    "# test data\n",
    "axes[1].scatter(y_test, y_test_pred, color = \"green\")\n",
    "axes[1].plot(y_train, y_train, color = \"red\", linewidth = 2)\n",
    "axes[1].set_xlabel(\"True values of the Response Variable (Test)\")\n",
    "axes[1].set_ylabel(\"Predicted values of the Response Variable (Test)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance is somewhat similar and consistent for both train and test dataset, while and the mean squared error for both train and test dataset is  similiar and consistent.\n",
    "\n",
    "The explained variance (train: 0.022, test: 0.014) is very low (almost 0) when compared to the other 3 variables. The mean squared error (train: 404,163,022, test: 396,198,430) is also very high, even higher than 'kills'. This shows that 'structures' is not a good predictor for 'totalgold'.\n",
    "\n",
    "This may be due to the almost horizontal spread of the data points as seen in the scatter plots above.\n",
    "\n",
    "This is the same finding as the finding from the joint plot section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Out of the 4 game variables, Total Minion Score ('totalcs') is the most suitable to predict the Total Combined Gold follows by Neutral Objectives ('neutralObjectives').\n",
    "\n",
    "Total Kills ('kills') is not a very good predictor, while Structures ('structures') should not be used to predict Total Combined Gold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration - Problem Definition #3\n",
    "Can we accurately predict the outcome of the match? Based on team compositions, total gold earned, kill score, creep scores, neutral objectives score, and structures taken score.\n",
    "\n",
    "Combining the data sets we have prepared from both our problems 1 & 2 and referring back to our EDA's we can use these data analysis and come up a machine learning model that is able to accurately predict the outcome of a game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning algorithm we chose to do is the Random Tree Classifier. It is a popular supervised machine learning algorithm where it is able to solve regression for (numeric target variable) and classification (categorical target variable) problems. \n",
    "\n",
    "Random forests tree works by creating multiple decision trees and combining them to provide the best most accurate prediction. \n",
    "\n",
    "We then implored Hypertuning Parameters method finding out the best variable to use to generate the number of trees and depth. \n",
    "\n",
    "To get this variable we used:\n",
    "\n",
    "Randomized Search:\n",
    " - Suitable for larger hyperparameter spaces.\n",
    " - May not guarantee finding the optimal combination, but it's more efficient in many cases.\n",
    " \n",
    "Grid Search\n",
    " - Exhaustively searches through all possible combinations of hyperparameters defined in a grid.\n",
    " - Suitable for smaller hyperparameter spaces where it's feasible to explore all combinations.\n",
    " - Guarantees finding the optimal combination if it exists within the search space.\n",
    "\n",
    "\n",
    "This 2 variables are then found via Random Search and Grid Search. We used both and analyzed which is better. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode champion columns\n",
    "champion_columns = ['t1p1_champion', 't1p2_champion', 't1p3_champion', 't1p4_champion', 't1p5_champion',\n",
    "                    't2p1_champion', 't2p2_champion', 't2p3_champion', 't2p4_champion', 't2p5_champion','t1p1_class', 't1p2_class', 't1p3_class', 't1p4_class', 't1p5_class',\n",
    "                    't2p1_class', 't2p2_class', 't2p3_class', 't2p4_class', 't2p5_class']\n",
    "\n",
    "\n",
    "merged_data_loldataQtn2_columnsUpdated = pd.read_csv('datasets/cleaned_dataset_matchWinner.csv', sep=',')\n",
    "\n",
    "\n",
    "merged_data_loldataQtn2_columnsUpdated = pd.get_dummies(merged_data_loldataQtn2_columnsUpdated, columns=champion_columns)\n",
    "\n",
    "# Convert matchWinner values to numeric\n",
    "# merged_data_loldataQtn2_columnsUpdated['matchWinner'] = merged_data_loldataQtn2_columnsUpdated['matchWinner'].astype(int)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = merged_data_loldataQtn2_columnsUpdated.drop('matchWinner', axis=1)\n",
    "y = merged_data_loldataQtn2_columnsUpdated['matchWinner']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=1)\n",
    "\n",
    "# Initialize and train the Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning using Random Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'n_estimators': randint(10,50),\n",
    "              'max_depth': randint(1,20)}\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = best_rf.get_params()\n",
    "\n",
    "max_depth = best_hyperparameters['max_depth']\n",
    "n_estimators = best_hyperparameters['n_estimators']\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=max_depth, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics for Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with the best model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using matplotlib and seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Winners')\n",
    "plt.ylabel('True Winners')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "accuracy_rand = accuracy_score(y_test, y_pred)\n",
    "precision_rand = precision_score(y_test, y_pred)\n",
    "recall_rand = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Search Metric\")\n",
    "print(\"Accuracy:\", accuracy_rand)\n",
    "print(\"Precision:\", precision_rand)\n",
    "print(\"Recall:\", recall_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retuning the hyperparameters using Grid Search rather than Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30, 40, 50],  \n",
    "    'max_depth': list(range(1, 21))  \n",
    "}\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the Grid Search Object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the Grid Search Object to the Data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf_grid = grid_search.best_estimator_\n",
    " \n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_hyperparameters_grid = best_rf_grid.get_params()\n",
    "\n",
    "max_depth = best_hyperparameters_grid['max_depth']\n",
    "n_estimator = best_hyperparameters_grid['max_depth']\n",
    "\n",
    "for i in range(n_estimator):\n",
    "    tree = best_rf_grid.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=max_depth, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with the best model\n",
    "y_pred = best_rf_grid.predict(X_test)\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using matplotlib and seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Winners')\n",
    "plt.ylabel('True Winners')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_pred = best_rf_grid.predict(X_test)\n",
    "\n",
    "accuracy_grid = accuracy_score(y_test, y_pred)\n",
    "precision_grid = precision_score(y_test, y_pred)\n",
    "recall_grid = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Grid Search Metrics\")\n",
    "print(\"Accuracy:\", accuracy_grid)\n",
    "print(\"Precision:\", precision_grid)\n",
    "print(\"Recall:\", recall_grid)\n",
    "print(\"\\n\")\n",
    "print(\"Random Search Metrics\")\n",
    "print(\"Accuracy:\", accuracy_rand)\n",
    "print(\"Precision:\", precision_rand)\n",
    "print(\"Recall:\", recall_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This is the metrics that we have come up with and from the multiple runs we can see that Frid Search is generally better for the Random Tree Forest Classifier when tuning the hyperparameters. This is due to the fact that our dataset is not too big therefore allowing a higher computational power to be used when running the model. \n",
    "\n",
    "And with these results: \n",
    "\n",
    "Achieving roughly close to 100%: \n",
    "\n",
    "Accuracy - 97% means that approximately 96.58% of the predictions made by the model were correct\n",
    "Precision - 94% means that approximately 94.12% of the instances predicted as positive by the model were actually positive\n",
    "Recall - 100% means that the model correctly identified all positive instances in the data\n",
    "\n",
    "We can accurately predict an outcome of a game based on the variables that we have analyzed. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
